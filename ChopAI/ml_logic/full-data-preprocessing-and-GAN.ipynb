{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data preprocessing for C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Music and image imports\n",
    "from imageio import imwrite\n",
    "from music21 import converter, instrument, note, chord, converter\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting midi file to images and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From midi file to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def extractNote(element):\n",
    "    return int(element.pitch.ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def extractDuration(element):\n",
    "    return element.duration.quarterLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def get_notes(notes_to_parse):\n",
    "\n",
    "    \"\"\"\n",
    "    Get all the notes and chords from the midi files into a dictionary containing:\n",
    "        - Start: unit time at which the note starts playing\n",
    "        - Pitch: pitch of the note\n",
    "        - Duration: number of time units the note is played for\n",
    "    \"\"\"\n",
    "    durations = []\n",
    "    notes = []\n",
    "    start = []\n",
    "\n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, note.Note):\n",
    "            if element.isRest:\n",
    "                continue\n",
    "\n",
    "            start.append(element.offset)\n",
    "            notes.append(extractNote(element))\n",
    "            durations.append(extractDuration(element))\n",
    "\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            if element.isRest:\n",
    "                continue\n",
    "            for chord_note in element:\n",
    "                start.append(element.offset)\n",
    "                durations.append(extractDuration(element))\n",
    "                notes.append(extractNote(chord_note))\n",
    "\n",
    "    return {\"start\":start, \"pitch\":notes, \"dur\":durations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi2image(midi_path, output_folder_path, max_repetitions = float(\"inf\"), resolution = 0.25, lowerBoundNote = 21, upperBoundNote = 127, maxSongLength = 106):\n",
    "\n",
    "    \"\"\"\n",
    "    1) Transform a midi file into a set of images:\n",
    "        - Each image has a size of 106 (all notes between lowerBound and upperBound) x 106 time units (maxSongLength)\n",
    "        - One time unit corresponds to 0.25 (resolution) beat from the original music\n",
    "    2) Store images into the corresponding sub-folder (identified by music piece name) of the 'output_folder_path' folder\n",
    "    \"\"\"\n",
    "\n",
    "    output_folder = f\"{output_folder_path}{midi_path.split('/')[-1].replace('.mid', '')}\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    mid = converter.parse(midi_path)\n",
    "\n",
    "    instruments = instrument.partitionByInstrument(mid)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    try:\n",
    "        i=0\n",
    "        for instrument_i in instruments.parts:\n",
    "            notes_to_parse = instrument_i.recurse()\n",
    "\n",
    "            notes_data = get_notes(notes_to_parse)\n",
    "            if len(notes_data[\"start\"]) == 0:\n",
    "                continue\n",
    "\n",
    "            if instrument_i.partName is None:\n",
    "                data[\"instrument_{}\".format(i)] = notes_data\n",
    "                i+=1\n",
    "            else:\n",
    "                data[instrument_i.partName] = notes_data\n",
    "\n",
    "    except:\n",
    "        notes_to_parse = mid.flat.notes\n",
    "        data[\"instrument_0\"] = get_notes(notes_to_parse)\n",
    "\n",
    "    for instrument_name, values in data.items():\n",
    "\n",
    "        pitches = values[\"pitch\"]\n",
    "        durs = values[\"dur\"]\n",
    "        starts = values[\"start\"]\n",
    "\n",
    "        index = 0\n",
    "        while index < max_repetitions:\n",
    "            matrix = np.zeros((upperBoundNote-lowerBoundNote,maxSongLength))\n",
    "\n",
    "\n",
    "            for dur, start, pitch in zip(durs, starts, pitches):\n",
    "                dur = int(dur/resolution)\n",
    "                start = int(start/resolution)\n",
    "\n",
    "                if not start > index*(maxSongLength+1) or not dur+start < index*maxSongLength:\n",
    "                    for j in range(start,start+dur):\n",
    "                        if j - index*maxSongLength >= 0 and j - index*maxSongLength < maxSongLength:\n",
    "                            matrix[pitch-lowerBoundNote,j - index*maxSongLength] = 255\n",
    "\n",
    "            if matrix.any(): # If matrix contains no notes (only zeros) don't save it\n",
    "                output_filename = os.path.join(output_folder, midi_path.split('/')[-1].replace(\".mid\",f\"_{instrument_name}_{index}.png\"))\n",
    "                imwrite(output_filename,matrix.astype(np.uint8))\n",
    "                index += 1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From image to midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def column2notes(column, lowerBoundNote = 21):\n",
    "    notes = []\n",
    "    for i in range(len(column)):\n",
    "        if column[i] > 255/2:\n",
    "            notes.append(i+lowerBoundNote)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def updateNotes(newNotes, prevNotes, resolution = 0.25): \n",
    "    res = {} \n",
    "    for note in newNotes:\n",
    "        if note in prevNotes:\n",
    "            res[note] = prevNotes[note] + resolution\n",
    "        else:\n",
    "            res[note] = resolution\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2midi(image_path, lowerBoundNote = 21, resolution = 0.25):\n",
    "    \"\"\"\n",
    "    From an existing image:\n",
    "        - Convert to notes\n",
    "        - Save result as a midi file in the subfolder 'music_piece_name' of the 'data_output_sound' folder \n",
    "    \"\"\"\n",
    "    \n",
    "    output_folder = f\"../../data_output_midi/{image_path.split('/')[-2]}\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    output_filename = os.path.join(output_folder, image_path.split(\"/\")[-1].replace(\".png\",\".mid\"))\n",
    "    print(output_filename)\n",
    "    \n",
    "    with ImageOps.grayscale(Image.open(image_path)) as image:\n",
    "        im_arr = np.frombuffer(image.tobytes(), dtype=np.uint8)\n",
    "        print(im_arr.shape)\n",
    "        try:\n",
    "            im_arr = im_arr.reshape((image.size[1], image.size[0]))\n",
    "        except:\n",
    "            im_arr = im_arr.reshape((image.size[1], image.size[0],3))\n",
    "            im_arr = np.dot(im_arr, [0.33, 0.33, 0.33])\n",
    "    \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "\n",
    "    prev_notes = updateNotes(im_arr.T[0,:],{}, resolution = resolution)\n",
    "    for column in im_arr.T[1:,:]:\n",
    "        notes = column2notes(column, lowerBoundNote=lowerBoundNote)\n",
    "        # pattern is a chord\n",
    "        notes_in_chord = notes\n",
    "        old_notes = prev_notes.keys()\n",
    "        for old_note in old_notes:\n",
    "            if not old_note in notes_in_chord:\n",
    "                new_note = note.Note(old_note,quarterLength=prev_notes[old_note])\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                if offset - prev_notes[old_note] >= 0:\n",
    "                    new_note.offset = offset - prev_notes[old_note]\n",
    "                    output_notes.append(new_note)\n",
    "                elif offset == 0:\n",
    "                    new_note.offset = offset\n",
    "                    output_notes.append(new_note)                    \n",
    "                else:\n",
    "                    print(offset,prev_notes[old_note],old_note)\n",
    "\n",
    "        prev_notes = updateNotes(notes_in_chord,prev_notes)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += resolution\n",
    "\n",
    "    for old_note in prev_notes.keys():\n",
    "        new_note = note.Note(old_note,quarterLength=prev_notes[old_note])\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        new_note.offset = offset - prev_notes[old_note]\n",
    "\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    prev_notes = updateNotes(notes_in_chord,prev_notes)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    \n",
    "    midi_stream.write('midi', fp=output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From midi files, create a clean image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_midi_data_as_images(midi_path, output_folder_path, image_height = 106, image_length = 106):\n",
    "\n",
    "    \"\"\"\n",
    "    Iterate on all midi files from the 'midi_path' folder to:\n",
    "        - Keep music pieces with one piano only\n",
    "        - Transform the midi file into images\n",
    "        - Store all corresponding images into a 'music_piece' subfolder of the 'output_folder_path'\n",
    "    \"\"\"\n",
    "    # Storing all midi files into a 'files_raw' list\n",
    "    files_raw = [file for file in os.listdir(midi_path)]\n",
    "\n",
    "    # Storing all midi files with only one piano in a 'files' list\n",
    "    files = []\n",
    "    for file in files_raw:\n",
    "        try:\n",
    "            mid = converter.parse(f'{midi_path}/{file}')\n",
    "            file_instruments = instrument.partitionByInstrument(mid)\n",
    "            if len(file_instruments)==1:\n",
    "                files.append(file)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Iterating on all files from 'files' list to create images\n",
    "    for file in files:\n",
    "        file_path = f\"{midi_path}/{file}\"\n",
    "        midi2image(file_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_images(input_path, output_path, height_image = 106, length_image = 106):\n",
    "    \"\"\"\n",
    "    Iterate on all images created in the 'input_path' folder:\n",
    "        - Resize images to height_image x length_image\n",
    "        - Transform them into pure black and white images\n",
    "        - Save them in a 'music piece' subfolder of the 'output_path' folder\n",
    "       \n",
    "    --> Input path: path to folder with input images (e.g., '../../data_test/Input_image')\n",
    "    --> Output path: path to folder where we wish to save output reshaped images (e.g., '../../data_test/Input_image_cleaned')\n",
    "    \"\"\"\n",
    "    \n",
    "    for music in os.listdir(input_path):\n",
    "        \n",
    "        output_folder = f'{output_path}/{music}' # Creating one sub_folder for each music piece in the 'output_path' folder\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        for image in os.listdir(f\"{input_path}/{music}\"):\n",
    "            image_path = f'{input_path}/{music}/{image}'\n",
    "            image_read = Image.open(image_path) # Reading each image\n",
    "            new_image = image_read.resize((106,106)) # Resizing each image\n",
    "            new_image = new_image.convert(\"1\") # Convert each image to pure black and white\n",
    "            new_image.save(f'{output_folder}/{image}') # Saving each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get clean array dataset from clean image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_array(input_path): \n",
    "    \"\"\"\n",
    "    Generate an array containing all images from 'input_path' folder in array format\n",
    "    \n",
    "    --> input_path = path of the folder containing clean images (e.g., '../../data_image_cleaned') \n",
    "    \"\"\"\n",
    "    \n",
    "    pixels = []\n",
    "    for music in os.listdir(input_path):\n",
    "        for image in os.listdir(f\"{input_path}/{music}\"):\n",
    "            image_path = f'{input_path}/{music}/{image}'\n",
    "            image_read = Image.open(image_path) # Reading each image\n",
    "            pixels_image = np.array(image_read.getdata()).astype('float32') # Store all pixel values in an array, each i_th-sequence contains the values of pixels in a i_th-row\n",
    "            pixels_image = pixels_image / 255.0 # All the values are 0 (black) and white (255). Normalize pixel values to be between 0 and 1\n",
    "            pixels.append(pixels_image.reshape(106, 106,1)) # Reshape pixels to be a matrix\n",
    "\n",
    "    pixels = np.array(pixels)\n",
    "    \n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path ='../../data_raw/'\n",
    "output_folder_path = '../../data_image/'\n",
    "get_clean_midi_data_as_images(midi_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../../data_image'\n",
    "output_path = '../../data_image_cleaned'\n",
    "clean_images(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1425, 106, 106, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "input_path = '../../data_image_cleaned'\n",
    "pixels = get_pixels_array(input_path)\n",
    "pixels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 16:34:16.758754: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-14 16:34:16.760926: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-14 16:34:16.803471: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-14 16:34:16.804456: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-14 16:34:17.736852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input,Dense, Reshape, Flatten, BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Dropout\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_image = 106\n",
    "length_image = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 106, 106, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 53, 53, 64)        1088      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 53, 53, 64)        0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 53, 53, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 27, 27, 64)        65600     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 27, 27, 64)        0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 27, 27, 64)        0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 46656)             0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 46656)             186624    \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 46657     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 299969 (1.14 MB)\n",
      "Trainable params: 206657 (807.25 KB)\n",
      "Non-trainable params: 93312 (364.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#the input is an image (black and white) with 106x106 pixels\n",
    "input_shape = (length_image, height_image, 1)\n",
    "inputs = Input(input_shape)\n",
    "\n",
    "#block1\n",
    "convolutional_layer_1 = Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=input_shape) (inputs)\n",
    "activation_1 = LeakyReLU(alpha=0.2) (convolutional_layer_1)\n",
    "dropout_1 = Dropout(0.5) (activation_1)\n",
    "\n",
    "#block2\n",
    "convolutional_layer_2 = Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=input_shape) (dropout_1)\n",
    "activation_2 = LeakyReLU(alpha=0.2) (convolutional_layer_2)\n",
    "dropout_2 = Dropout(0.5) (activation_2)\n",
    "\n",
    "flattened_layer = Flatten()(dropout_2)\n",
    "batch_normalization_layer = BatchNormalization()(flattened_layer)\n",
    "output_discriminator = Dense(1, activation=\"sigmoid\")(batch_normalization_layer)\n",
    "\n",
    "discriminator_model = Model(inputs, outputs=output_discriminator)\n",
    "\n",
    "discriminator_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/romainlebihan/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/romainlebihan/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages (from pydot) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "! pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimension = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 359552)            36314752  \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 359552)            0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 53, 53, 128)       0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 53, 53, 1024)      132096    \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 106, 106, 1024)    16778240  \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 106, 106, 1024)    1049600   \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 106, 106, 1024)    0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 106, 106, 1024)    1049600   \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 106, 106, 1)       50177     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55374465 (211.24 MB)\n",
      "Trainable params: 55374465 (211.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (latent_dimension)\n",
    "inputs = Input(input_shape)\n",
    "\n",
    "dense_1 = Dense(128*53*53, input_dim=latent_dimension)(inputs)\n",
    "activation_1 = LeakyReLU(alpha=0.2)(dense_1)\n",
    "reshape_layer = Reshape( (53,53,128))(activation_1)\n",
    "\n",
    "dense_2 = Dense(1024)(reshape_layer)\n",
    "conv2d_transposed_layer_1 = Conv2DTranspose(1024,(4,4), strides=(2,2), padding=\"same\")(dense_2)\n",
    "\n",
    "dense_3 = Dense(1024)(conv2d_transposed_layer_1)\n",
    "activation_2 = LeakyReLU(alpha=0.2)(dense_3)\n",
    "dense_4 = Dense(1024)(activation_2)\n",
    "conv2d_transposed_layer_1 = Conv2DTranspose(1,(7,7), padding=\"same\", activation='sigmoid')(dense_4)\n",
    "\n",
    "generator_model = Model(inputs, outputs=conv2d_transposed_layer_1)\n",
    "\n",
    "generator_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN (consolidation of generator and discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_3 (Functional)        (None, 106, 106, 1)       55374465  \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 1)                 299969    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55674434 (212.38 MB)\n",
      "Trainable params: 55374465 (211.24 MB)\n",
      "Non-trainable params: 299969 (1.14 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_model.trainable = False\n",
    "GAN = Sequential()\n",
    "GAN.add(generator_model)\n",
    "GAN.add(discriminator_model)\n",
    "\n",
    "GAN.compile(loss='binary_crossentropy', optimizer= Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_music_samples(pixels, num_samples):\n",
    "  #generate num_samples random integer number between 0 and the size of total dataset of pixels\n",
    "  #these numbers/indexes will be the indexes of converted image that we will use as real music sample\n",
    "  indexes_real_image = np.random.randint(0, pixels.shape[0], num_samples)\n",
    "  real_choosen_images = pixels[indexes_real_image]\n",
    "  #create a ground truth of 1 for each of the images (due to the fact are real images)\n",
    "  ground_truth_choosen_images = np.ones( (num_samples, 1))\n",
    "  return real_choosen_images, ground_truth_choosen_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_samples(latent_dimension, num_samples):\n",
    "  #generate latent_dimension*num_samples array of random values taken from x axis of Normal Distribution\n",
    "  latent_samples = np.random.randn(latent_dimension*num_samples)\n",
    "  #reshape to have num_samples entries each one with latent_dimension values\n",
    "  latent_samples = latent_samples.reshape(num_samples, latent_dimension)\n",
    "  return latent_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a useful function to compute accuracy of the discriminator in predicting correctly both real and fake music samples\n",
    "\n",
    "def show_current_discriminator_accuracy(discriminator_model, generator_model, pixels, latent_dimension):\n",
    "\n",
    "  num_samples_to_test = 100\n",
    "\n",
    "  #generate real music samples\n",
    "  real_music_samples, ground_truth_real_music_samples = generate_real_music_samples(pixels, num_samples_to_test)\n",
    "  \n",
    "  #generate fake music samples\n",
    "  latent_samples = generate_latent_samples(latent_dimension, num_samples_to_test)\n",
    "  images_predicted_from_generator = generator_model.predict(latent_samples)\n",
    "  #create a zero ground truth (because are no real images)\n",
    "  ground_truth_images_predicted_from_generator = np.zeros( (num_samples_to_test, 1))\n",
    "\n",
    "  #evaluate the accuracy of the discriminator on real music samples\n",
    "  _, accuracy_on_real = discriminator_model.evaluate(real_music_samples, ground_truth_real_music_samples, verbose=0)\n",
    "  #evaluate the accuracy of the discriminator on fake music samples\n",
    "  _, accuracy_on_fake = discriminator_model.evaluate(images_predicted_from_generator, ground_truth_images_predicted_from_generator, verbose=0)\n",
    "\n",
    "  #print results\n",
    "  print(\"   Current accuracy of the discriminator on real music samples:\", round(accuracy_on_real*100,3),\"%\")\n",
    "  print(\"   Current accuracy of the discriminator on fake music samples:\", round(accuracy_on_fake*100,3),\"% \\n\")\n",
    "\n",
    "  return accuracy_on_real, accuracy_on_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " epoch: 0\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "   batch: 0    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 1.000048279762268    #samples to train generator: 25    #samples to train discriminator: 1\n",
      "   batch: 1    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.8907220363616943    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 2    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6909752488136292    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 3    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6894126534461975    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 4    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6891672015190125    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 5    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6890504360198975    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 6    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6895810961723328    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 7    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6889271140098572    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 8    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6881256699562073    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 9    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.687939465045929    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 10    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6870220303535461    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 11    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6863161325454712    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 12    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6854462027549744    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 13    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6801604628562927    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "   batch: 14    discriminator_loss_on_real_music: 0.35084646940231323    discriminator_loss_on_fake_images: 0.6765531301498413    #samples to train generator: 23    #samples to train discriminator: 0\n",
      "4/4 [==============================] - 161s 37s/step\n",
      "   Current accuracy of the discriminator on real music samples: 3.0 %\n",
      "   Current accuracy of the discriminator on fake music samples: 9.0 % \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22170/591351528.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;31m#print the current accuracy obtained in classifing correctly both real music samples and fake music samples (the ones generated by the generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0maccuracy_on_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_on_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_current_discriminator_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mdiscriminator_info_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_info_per_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss_discriminator_on_real_music'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss_generator_on_fake_music'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGAN_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_on_real'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccuracy_on_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_on_fake'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccuracy_on_fake\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6200\u001b[0m         ):\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 250\n",
    "number_of_batch_per_epoch = 15\n",
    "number_of_samples_per_batch = int((pixels.shape[0] / number_of_batch_per_epoch)*0.04)\n",
    "number_of_samples_to_take_per_batch = int(number_of_batch_per_epoch / 2)\n",
    "\n",
    "#this array contains one image for each epoch that was generated by the generator. In this way is possible to \"see\" how the generations changes\n",
    "images_generated_per_epoch = []\n",
    "\n",
    "#here i will store loss and accuracy information of the discriminator during the epochs\n",
    "discriminator_info_per_epoch = pd.DataFrame(columns=['loss_discriminator_on_real_music', 'loss_generator_on_fake_music', 'accuracy_on_real', 'accuracy_on_fake'])  \n",
    "\n",
    "accuracy_on_fake = 0\n",
    "accuracy_on_real = 0\n",
    "\n",
    "num_samples_for_generator = number_of_samples_per_batch*2\n",
    "num_samples_for_discriminator = number_of_samples_per_batch\n",
    "\n",
    "#for each epoch...\n",
    "for id_epoch in range(number_of_epochs):\n",
    "  print(\"\\n\\n epoch:\", id_epoch)\n",
    "\n",
    "  \n",
    "  #da eliminare (serve solo per fare le epoche da 199 a 500)\n",
    "  if id_epoch == 0:\n",
    "    num_samples_for_generator = 25\n",
    "    num_samples_for_discriminator = 1\n",
    "    accuracy_on_fake = 95\n",
    "    accuracy_on_real = 95\n",
    "  \n",
    "  #for each batch\n",
    "  for id_batch in range(number_of_batch_per_epoch):\n",
    "\n",
    "    if num_samples_for_discriminator > 0:\n",
    "      #take real music samples from pixels database (the ones obtained after conversion from midi to image and after pure black and white conversion)\n",
    "      real_music_samples, ground_truth_real_music_samples = generate_real_music_samples(pixels, num_samples_for_discriminator)\n",
    "\n",
    "      #generate lantent samples\n",
    "      latent_samples = generate_latent_samples(latent_dimension, num_samples_for_discriminator )\n",
    "\n",
    "      #use the generator to predict an image giving lantent samples\n",
    "      images_generated_from_generator = generator_model.predict(latent_samples)\n",
    "      images_generated_per_epoch.append(images_generated_from_generator[0])\n",
    "\n",
    "      #create a zero ground truth (because are no real images)\n",
    "      ground_truth_images_generated_from_generator = np.zeros( (num_samples_for_discriminator, 1))\n",
    "\n",
    "      #create the input samples to fed to discriminatore which are made up of both real and fake music samples\n",
    "      discriminator_inputs = np.vstack( (real_music_samples, images_generated_from_generator ))\n",
    "      discriminator_ground_truth = np.vstack( (ground_truth_real_music_samples,ground_truth_images_generated_from_generator )) \n",
    "\n",
    "\n",
    "      #train the discriminator (the one alone) on this current batch\n",
    "      discriminator_loss, _ = discriminator_model.train_on_batch(discriminator_inputs,discriminator_ground_truth)\n",
    "\n",
    "    '''\n",
    "      Now we force the discriminator to stop learning more. The GAN will now have an overtrained discriminator, which however can not be longer trained. \n",
    "      Now we generate latent samples that we mark as real (even if they are not), we take the images that the GAN generator has generated, \n",
    "      and we feed them to the discriminator. Obviously, being this \"better\" than the generator will classify them as fake. \n",
    "      By forcing the GAN to classify them as true instead, we will adjust the generator weights in such a way as to generate images, given \n",
    "      the same random latent samples, that can \"make fun\" of the discriminator. Note that the discriminator weights will not be affected.\n",
    "    '''\n",
    "\n",
    "\n",
    "    #generate latent samples (the double of before)\n",
    "    latent_samples = generate_latent_samples(latent_dimension, num_samples_for_generator)\n",
    "    #mark them as real (even if they are not)\n",
    "    ground_truth_latent_samples = np.ones( (num_samples_for_generator,1) )\n",
    "    #train the GAN (so, only the generator)\n",
    "    GAN_loss = GAN.train_on_batch(latent_samples, ground_truth_latent_samples)\n",
    "\n",
    "    #print some information related with the training\n",
    "    print(\"   batch:\", id_batch, \"   discriminator_loss_on_real_music:\", discriminator_loss, \"   discriminator_loss_on_fake_images:\", GAN_loss, \"   #samples to train generator:\", num_samples_for_generator,  \"   #samples to train discriminator:\", num_samples_for_discriminator)\n",
    "\n",
    "    #this control is intended to train the generator until it reaches a level almost equal to that of the discriminator. If it were not there it would be impossible for the generator to reach the discriminator as this seems to learn very quickly.\n",
    "    if accuracy_on_real > 0.8 and accuracy_on_fake > 0.6:\n",
    "      num_samples_for_generator = int((pixels.shape[0] / number_of_batch_per_epoch)*0.25)\n",
    "      num_samples_for_discriminator = 0\n",
    "    else:\n",
    "      num_samples_for_generator = number_of_samples_per_batch*2\n",
    "      num_samples_for_discriminator = number_of_samples_per_batch \n",
    "\n",
    "\n",
    "  #print the current accuracy obtained in classifing correctly both real music samples and fake music samples (the ones generated by the generator)\n",
    "  accuracy_on_real, accuracy_on_fake = show_current_discriminator_accuracy(discriminator_model, generator_model, pixels, latent_dimension)\n",
    "  discriminator_info_per_epoch = discriminator_info_per_epoch.append({'loss_discriminator_on_real_music':  discriminator_loss, 'loss_generator_on_fake_music': GAN_loss, 'accuracy_on_real': accuracy_on_real, 'accuracy_on_fake': accuracy_on_fake}, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChopAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
