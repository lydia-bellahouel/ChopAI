{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import pickle\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Music\n",
    "import music21 as m21\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "\n",
    "# Data Visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Misc\n",
    "import pickle\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Music\n",
    "import music21 as m21\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "\n",
    "# Data Visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_folders(source_dir = \"../../data_raw/\", train_percentage = 90):\n",
    "\n",
    "    \"\"\"\n",
    "    Create train / test folders\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the percentage of data to use to training the model\n",
    "\n",
    "\n",
    "    train_dir = \"../../data_split/train/\"\n",
    "    test_dir = \"../../data_split/test/\"\n",
    "\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    all_files = os.listdir(source_dir)\n",
    "\n",
    "    num_train_files = int(len(all_files) * (train_percentage / 100))\n",
    "\n",
    "\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    # Split the files into training and testing sets\n",
    "    train_files = all_files[:num_train_files]\n",
    "    test_files = all_files[num_train_files:]\n",
    "\n",
    "\n",
    "    for file in train_files:\n",
    "        source_file = os.path.join(source_dir, file)\n",
    "        destination_file = os.path.join(train_dir, file)\n",
    "        shutil.copy(source_file, destination_file)\n",
    "\n",
    "\n",
    "    for file in test_files:\n",
    "        source_file = os.path.join(source_dir, file)\n",
    "        destination_file = os.path.join(test_dir, file)\n",
    "        shutil.copy(source_file, destination_file)\n",
    "\n",
    "    print(\"✅ train & test folders created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train & test folders created\n"
     ]
    }
   ],
   "source": [
    "create_train_test_folders(source_dir = \"../../data_raw/\", train_percentage = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_into_notes():\n",
    "    \"\"\"Convert train files & test files into notes\"\"\"\n",
    "\n",
    "    # Extracted notes for each file\n",
    "    notes_train = []\n",
    "    notes_test = []\n",
    "\n",
    "    # Save the indexes of notes in which a new composition started to create consistent sequences\n",
    "    new_composition_indexes_train = []\n",
    "    new_composition_indexes_test = []\n",
    "\n",
    "\n",
    "    # Load all train MIDI files into notes\n",
    "    input_path_train = \"../../data_split/train\"\n",
    "    for file in os.listdir(input_path_train):\n",
    "        file_path = os.path.join(input_path_train, file)\n",
    "\n",
    "        # Add new index to alert that it's a new composition\n",
    "        if new_composition_indexes_train and new_composition_indexes_train[-1] != len(notes_train):\n",
    "            new_composition_indexes_train.append(len(notes_train))\n",
    "\n",
    "        try:\n",
    "            # Convert the music into a score object\n",
    "            score = converter.parse(file_path)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            elements_in_part = None\n",
    "\n",
    "            try:  # File has instrument parts\n",
    "                # Given a score that represents the MIDI, partition it into parts for each unique instrument found\n",
    "                partitions_by_instrument = instrument.partitionByInstrument(score)\n",
    "                # Visit all the elements (notes, chords, rests, and more) of each of its internal \"measures.\"\n",
    "                elements_in_part = partitions_by_instrument.parts[0].recurse()\n",
    "\n",
    "            except:  # File has notes in a flat structure\n",
    "                elements_in_part = score.flat.notes\n",
    "\n",
    "            # Scroll through all the elements (notes or chords) picked up\n",
    "            for element in elements_in_part:\n",
    "                # If the element is a note...\n",
    "                if isinstance(element, note.Note):\n",
    "                    # Add note to array\n",
    "                    notes_train.append(str(element.pitch))\n",
    "                # If the element is a chord (a set of notes --> e.g., C4 F4)\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    # Extract each note from the chord and insert it into the array in the format Note1.Note2.Note3\n",
    "                    notes_train.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    # Load all test MIDI files into notes\n",
    "    input_path_test = \"../../data_split/test\"\n",
    "    for file in os.listdir(input_path_test):\n",
    "        file_path = os.path.join(input_path_test, file)\n",
    "\n",
    "        # Add new index to alert that it's a new composition\n",
    "        if new_composition_indexes_test and new_composition_indexes_test[-1] != len(notes_test):\n",
    "            new_composition_indexes_test.append(len(notes_test))\n",
    "\n",
    "        try:\n",
    "            # Convert the music into a score object\n",
    "            score = converter.parse(file_path)\n",
    "\n",
    "            print(\"Parsing %s\" % file)\n",
    "\n",
    "            elements_in_part = None\n",
    "\n",
    "            try:  # File has instrument parts\n",
    "                # Given a score that represents the MIDI, partition it into parts for each unique instrument found\n",
    "                partitions_by_instrument = instrument.partitionByInstrument(score)\n",
    "                # Visit all the elements (notes, chords, rests, and more) of each of its internal \"measures.\"\n",
    "                elements_in_part = partitions_by_instrument.parts[0].recurse()\n",
    "\n",
    "            except:  # File has notes in a flat structure\n",
    "                elements_in_part = score.flat.notes\n",
    "\n",
    "            # Scroll through all the elements (notes or chords) picked up\n",
    "            for element in elements_in_part:\n",
    "                # If the element is a note...\n",
    "                if isinstance(element, note.Note):\n",
    "                    # Add note to array\n",
    "                    notes_test.append(str(element.pitch))\n",
    "                # If the element is a chord (a set of notes --> e.g., C4 F4)\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    # Extract each note from the chord and insert it into the array in the format Note1.Note2.Note3\n",
    "                    notes_test.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    print(\"✅Loading notes done for train & test files\")\n",
    "\n",
    "    # Save the 'notes_train' and 'notes_test' list to a pickle file\n",
    "    os.makedirs(\"../../data_vocab/\")\n",
    "    with open(\"../../data_vocab/notes_train.pkl\", \"wb\") as filepath:\n",
    "        pickle.dump(notes_train, filepath)\n",
    "    with open(\"../../data_vocab/notes_test.pkl\", \"wb\") as filepath:\n",
    "        pickle.dump(notes_test, filepath)\n",
    "\n",
    "    # Return notes and new composition indexes\n",
    "    return notes_train, notes_test, new_composition_indexes_train, new_composition_indexes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing mazrka35.mid\n",
      "Parsing dgffc49.mid\n",
      "Parsing rondo73.mid\n",
      "Parsing contreda.mid\n",
      "Parsing mk_chim2.mid\n",
      "Parsing chonoc14.mid\n",
      "Parsing choval12.mid\n",
      "Parsing chonoc17.mid\n",
      "Parsing mazrka11.mid\n",
      "Parsing mazrka04.mid\n",
      "Parsing mazrka06.mid\n",
      "Parsing chpnimpu.mid\n",
      "Parsing chopineb.mid\n",
      "Parsing waltz_am.mid\n",
      "Parsing chetude1.mid\n",
      "Parsing mazrka32.mid\n",
      "Parsing mazrka22.mid\n",
      "Parsing chopol12.mid\n",
      "Parsing mazrka27.mid\n",
      "Parsing mazrka09.mid\n",
      "Parsing mazrka44.mid\n",
      "Parsing mazrka41.mid\n",
      "Parsing mazrka43.mid\n",
      "Error parsing lecpsc3.mid: badly formatted midi bytes, got: b'RIFF\\x0eN\\x00\\x00RMIDdata\\x01N\\x00\\x00'\n",
      "Parsing chlargo.mid\n",
      "Parsing mazrka12.mid\n",
      "Parsing choval13.mid\n",
      "Parsing lecpsb3.mid\n",
      "Parsing choval05.mid\n",
      "Parsing chopol05.mid\n",
      "Parsing pologm.mid\n",
      "Parsing fugaam.mid\n",
      "Parsing chopol08.mid\n",
      "Parsing prelop45.mid\n",
      "Parsing nocturne.mid\n",
      "Parsing mazrka28.mid\n",
      "Parsing op72.mid\n",
      "Parsing chpolfnt.mid\n",
      "Parsing mazrka03.mid\n",
      "Parsing varigerm.mid\n",
      "Parsing chonoc08.mid\n",
      "Parsing souvpaga.mid\n",
      "Parsing chonoc12.mid\n",
      "Parsing mazrka38.mid\n",
      "Parsing mazrka08.mid\n",
      "Parsing ch_pre20.mid\n",
      "Parsing berceuse.mid\n",
      "Parsing choschz2.mid\n",
      "Parsing chp4iapb.mid\n",
      "Parsing chonoc21.mid\n",
      "Parsing skchonoc.mid\n",
      "Parsing mazrka29.mid\n",
      "Parsing mazrka16.mid\n",
      "Parsing mazrka37.mid\n",
      "Parsing chonoc07.mid\n",
      "Parsing mazrka07.mid\n",
      "Parsing choval11.mid\n",
      "Parsing chonoc15.mid\n",
      "Parsing mazrka51.mid\n",
      "Parsing chopin-1.mid\n",
      "Parsing mazrka24.mid\n",
      "Parsing chopol16.mid\n",
      "Parsing mazrka34.mid\n",
      "Parsing chonoc16.mid\n",
      "Parsing mazrka14.mid\n",
      "Parsing ballade3.mid\n",
      "Parsing chopol09.mid\n",
      "Parsing chopol13.mid\n",
      "Parsing ch_wal03.mid\n",
      "Parsing mazrka45.mid\n",
      "Parsing choval17.mid\n",
      "Parsing mazrka17.mid\n",
      "Parsing mazrka39.mid\n",
      "Error parsing lechi4.mid: badly formatted midi bytes, got: b'RIFF*\\xa4\\x00\\x00RMIDdata\\x1e\\xa4\\x00\\x00'\n",
      "Parsing chpnbarc.mid\n",
      "Parsing choval16.mid\n",
      "Parsing nouv3.mid\n",
      "Parsing lecpsa3.mid\n",
      "Parsing chonoc11.mid\n",
      "Parsing mazrka25.mid\n",
      "Parsing chopin_e.mid\n",
      "Parsing ballade2.mid\n",
      "Parsing mazrka30.mid\n",
      "Parsing chophex4.mid\n",
      "Parsing chv8pbi3.mid\n",
      "Parsing waltzem.mid\n",
      "Parsing valse_n.mid\n",
      "Parsing scherzo1.mid\n",
      "Parsing mazrka01.mid\n",
      "Parsing chop23b.mid\n",
      "Parsing mazrka13.mid\n",
      "Parsing op32-1.mid\n",
      "Parsing mazrka33.mid\n",
      "Parsing mazrka18.mid\n",
      "Parsing mazrka10.mid\n",
      "Parsing chp2i8pb.mid\n",
      "Parsing mazrka50.mid\n",
      "Parsing mazrka31.mid\n",
      "Parsing mazrka21.mid\n",
      "Parsing mazrka05.mid\n",
      "Parsing cmazurka.mid\n",
      "Parsing mazrka20.mid\n",
      "Parsing choval15.mid\n",
      "Parsing algrcrt3.mid\n",
      "Parsing chsc3.mid\n",
      "Parsing funeralm.mid\n",
      "Parsing mazrka19.mid\n",
      "Parsing chopol10.mid\n",
      "Parsing noct15-1.mid\n",
      "Parsing mazurka.mid\n",
      "Parsing mazrka02.mid\n",
      "Parsing chonoc18.mid\n",
      "Parsing chopol14.mid\n",
      "Parsing chopol15.mid\n",
      "Parsing chopnpol.mid\n",
      "Parsing lecpsd3.mid\n",
      "Parsing variludo.mid\n",
      "Parsing mazrka15.mid\n",
      "Parsing mazrka36.mid\n",
      "Parsing chonoc13.mid\n",
      "Parsing mazrka23.mid\n",
      "Parsing mazrka40.mid\n",
      "Parsing mazrka42.mid\n",
      "Parsing mazurka_.mid\n",
      "Parsing mazrka26.mid\n",
      "✅Loading notes done for train & test files\n"
     ]
    }
   ],
   "source": [
    "notes_train, notes_test, new_composition_indexes_train, new_composition_indexes_test = load_midi_into_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(notes_train):\n",
    "\n",
    "    \"\"\"define notes vocabulary based on train set\"\"\"\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames_train = sorted(set(item for item in notes_train))\n",
    "\n",
    "    # create a dictionary to map pitches to integers\n",
    "    note_to_int_train = dict((note, number) for number, note in enumerate(pitchnames_train))\n",
    "\n",
    "    n_vocab_train = len(set(notes_train))\n",
    "\n",
    "    return note_to_int_train, n_vocab_train, pitchnames_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_to_int_train, n_vocab_train, pitchnames_train =  create_vocabulary(notes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(notes_train, notes_test, new_composition_indexes_train, new_composition_indexes_test, note_to_int_train,\n",
    "                     n_vocab_train, pitchnames_train, sequence_length = 100):\n",
    "    \"\"\"\n",
    "    Prepare the test and train sequences to be used by the Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    # !!TRAIN!! split creation\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    wait = 0\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes_train) - sequence_length, 1):\n",
    "\n",
    "        # if the ground truth index is a note/chord that belongs to a new composition\n",
    "        if (i + sequence_length) in new_composition_indexes_train:\n",
    "            wait = sequence_length - 1\n",
    "            continue\n",
    "        if wait != 0:\n",
    "            wait = wait - 1\n",
    "            continue\n",
    "\n",
    "        sequence_in = notes_train[i:i + sequence_length]\n",
    "        sequence_out = notes_train[i + sequence_length]\n",
    "\n",
    "        X_train.append([note_to_int_train[char] for char in sequence_in])\n",
    "        y_train.append(note_to_int_train[sequence_out])\n",
    "\n",
    "    n_patterns = len(X_train)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    X_train = np.reshape(X_train, (n_patterns, sequence_length, 1))\n",
    "\n",
    "    # normalize input\n",
    "    X_train = X_train / float(n_vocab_train)\n",
    "\n",
    "    # one-hot encoding of the output\n",
    "    y_train = to_categorical(y_train, num_classes=n_vocab_train)\n",
    "\n",
    "\n",
    "    ####################\n",
    "    # !!TEST!! split creation\n",
    "    ####################\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    wait = 0\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes_test) - sequence_length, 1):\n",
    "\n",
    "        # if the ground truth index is a note/chord that belongs to a new composition\n",
    "        if (i + sequence_length) in new_composition_indexes_test:\n",
    "            wait = sequence_length - 1\n",
    "            continue\n",
    "        if wait != 0:\n",
    "            wait = wait - 1\n",
    "            continue\n",
    "\n",
    "        sequence_in = notes_test[i:i + sequence_length]\n",
    "        sequence_out = notes_test[i + sequence_length]\n",
    "\n",
    "        X_test.append([note_to_int_train.get(char, -1) for char in sequence_in])\n",
    "        if sequence_out in note_to_int_train.keys():\n",
    "            y_test.append(note_to_int_train[sequence_out])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    n_patterns = len(X_test)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    X_test = np.reshape(X_test, (n_patterns, sequence_length, 1))\n",
    "\n",
    "    # normalize input\n",
    "    X_test = X_test / float(n_vocab_train)\n",
    "\n",
    "    # one-hot encoding of the output\n",
    "    y_test = to_categorical(y_test, num_classes=n_vocab_train)\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(notes_train, notes_test, new_composition_indexes_train, new_composition_indexes_test, note_to_int_train,\n",
    "                     n_vocab_train, pitchnames_train, sequence_length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_compile_LSTM_model(X_train, n_vocab_train):\n",
    "    \"\"\"Define model architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256,\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        return_sequences=True,\n",
    "    ))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(n_vocab_train))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126391, 897)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_and_compile_LSTM_model(X_train, n_vocab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 100, 256)          264192    \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 897)               230529    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 897)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1020033 (3.89 MB)\n",
      "Trainable params: 1020033 (3.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train):\n",
    "\n",
    "    #checkpoints & callbacks:\n",
    "\n",
    "    os.makedirs(\"../../checkpoint_lstm\", exist_ok=True)\n",
    "    file_path = \"../../checkpoint_lstm/best_weights.h5\"\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        file_path,\n",
    "        monitor='accuracy',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    callbacks_list = [checkpoint,es]\n",
    "\n",
    "\n",
    "    # train model :\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=64 ,callbacks=callbacks_list, validation_split=0.2, verbose=1)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126391, 100, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 250/1580 [===>..........................] - ETA: 9:35 - loss: 5.2585 - accuracy: 0.0185"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train_model(model, X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m history\n",
      "\u001b[1;32m/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m callbacks_list \u001b[39m=\u001b[39m [checkpoint,es]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# train model :\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m ,callbacks\u001b[39m=\u001b[39;49mcallbacks_list, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/lydia/code/lydia-bellahouel/ChopAI/ChopAI/ml_logic/test_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model(model, X_train, y_train)\n",
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChopAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
