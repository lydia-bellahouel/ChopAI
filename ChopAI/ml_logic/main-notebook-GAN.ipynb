{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data preprocessing and model training for C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Music and image imports\n",
    "from imageio import imwrite\n",
    "from music21 import converter, instrument, note, chord, converter\n",
    "from PIL import Image, ImageOps\n",
    "import mido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting midi files into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def extractNote(element):\n",
    "    return int(element.pitch.ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def extractDuration(element):\n",
    "    return element.duration.quarterLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediary function\n",
    "def get_notes(notes_to_parse):\n",
    "\n",
    "    \"\"\"\n",
    "    Get all the notes and chords from the midi files into a dictionary containing:\n",
    "        - Start: unit time at which the note starts playing\n",
    "        - Pitch: pitch of the note\n",
    "        - Duration: number of time units the note is played for\n",
    "    \"\"\"\n",
    "    durations = []\n",
    "    notes = []\n",
    "    start = []\n",
    "\n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, note.Note):\n",
    "            if element.isRest:\n",
    "                continue\n",
    "\n",
    "            start.append(element.offset)\n",
    "            notes.append(extractNote(element))\n",
    "            durations.append(extractDuration(element))\n",
    "\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            if element.isRest:\n",
    "                continue\n",
    "            for chord_note in element:\n",
    "                start.append(element.offset)\n",
    "                durations.append(extractDuration(element))\n",
    "                notes.append(extractNote(chord_note))\n",
    "\n",
    "    return {\"start\":start, \"pitch\":notes, \"dur\":durations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi2image(midi_path, output_folder_path, max_repetitions = float(\"inf\"), resolution = 0.25, lowerBoundNote = 21, upperBoundNote = 127, maxSongLength = 100):\n",
    "\n",
    "    \"\"\"\n",
    "    1) Transform a midi file into a set of images:\n",
    "        - Each image has a size of 106 (all notes between lowerBound and upperBound) x 106 time units (maxSongLength)\n",
    "        - One time unit corresponds to 0.25 (resolution) beat from the original music\n",
    "    2) Store images into the corresponding sub-folder (identified by music piece name) of the 'output_folder_path' folder\n",
    "\n",
    "    --> midi_path: path to midi file (e.g., '../../data_test/Input_midi/ballade2.mid')\n",
    "    --> output_folder_path: path to folder where we wish to save created images (e.g., '../../data_test/Input_image/')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    output_folder = f\"{output_folder_path}{midi_path.split('/')[-1].replace('.mid', '')}\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    mid = converter.parse(midi_path)\n",
    "\n",
    "    instruments = instrument.partitionByInstrument(mid)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    try:\n",
    "        i=0\n",
    "        for instrument_i in instruments.parts:\n",
    "            notes_to_parse = instrument_i.recurse()\n",
    "\n",
    "            notes_data = get_notes(notes_to_parse)\n",
    "            if len(notes_data[\"start\"]) == 0:\n",
    "                continue\n",
    "\n",
    "            if instrument_i.partName is None:\n",
    "                data[\"instrument_{}\".format(i)] = notes_data\n",
    "                i+=1\n",
    "            else:\n",
    "                data[instrument_i.partName] = notes_data\n",
    "\n",
    "    except:\n",
    "        notes_to_parse = mid.flat.notes\n",
    "        data[\"instrument_0\"] = get_notes(notes_to_parse)\n",
    "\n",
    "    for instrument_name, values in data.items():\n",
    "\n",
    "        pitches = values[\"pitch\"]\n",
    "        durs = values[\"dur\"]\n",
    "        starts = values[\"start\"]\n",
    "\n",
    "        index = 0\n",
    "        while index < max_repetitions:\n",
    "            matrix = np.zeros((upperBoundNote-lowerBoundNote,maxSongLength))\n",
    "\n",
    "\n",
    "            for dur, start, pitch in zip(durs, starts, pitches):\n",
    "                dur = int(dur/resolution)\n",
    "                start = int(start/resolution)\n",
    "\n",
    "                if not start > index*(maxSongLength+1) or not dur+start < index*maxSongLength:\n",
    "                    for j in range(start,start+dur):\n",
    "                        if j - index*maxSongLength >= 0 and j - index*maxSongLength < maxSongLength:\n",
    "                            matrix[pitch-lowerBoundNote,j - index*maxSongLength] = 255\n",
    "\n",
    "            if matrix.any(): # If matrix contains no notes (only zeros) don't save it\n",
    "                output_filename = os.path.join(output_folder, midi_path.split('/')[-1].replace(\".mid\",f\"_{instrument_name}_{index}.png\"))\n",
    "                imwrite(output_filename,matrix.astype(np.uint8))\n",
    "                index += 1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "midi2image(\"../../data_test/Input_midi/ballade2.mid\", \"../../data_test/Input_image/\")\n",
    "midi2image(\"../../data_test/Input_midi/ballade3.mid\", \"../../data_test/Input_image/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From midi files, create a clean image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_midi_data_as_images(midi_path, output_folder_path, image_height = 106, image_length = 106):\n",
    "\n",
    "    \"\"\"\n",
    "    Iterate on all midi files from the 'midi_path' folder to:\n",
    "        - Keep music pieces with one piano only\n",
    "        - Transform the midi file into images\n",
    "        - Store all corresponding images into a 'music_piece' subfolder of the 'output_folder_path'\n",
    "    \"\"\"\n",
    "    # Storing all midi files into a 'files_raw' list\n",
    "    files_raw = [file for file in os.listdir(midi_path)]\n",
    "\n",
    "    # Storing all midi files with only one piano in a 'files' list\n",
    "    files = []\n",
    "    for file in files_raw:\n",
    "        try:\n",
    "            mid = converter.parse(f'{midi_path}/{file}')\n",
    "            file_instruments = instrument.partitionByInstrument(mid)\n",
    "            if len(file_instruments)==1:\n",
    "                files.append(file)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Iterating on all files from 'files' list to create images\n",
    "    for file in files:\n",
    "        file_path = f\"{midi_path}/{file}\"\n",
    "        midi2image(file_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "midi_path ='../../data_test/Input_midi/'\n",
    "output_folder_path = '../../data_test/Input_image/'\n",
    "get_clean_midi_data_as_images(midi_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_images(input_path, output_path, height_image = 106, length_image = 106):\n",
    "    \"\"\"\n",
    "    Iterate on all images created in the 'input_path' folder:\n",
    "        - Resize images to height_image x length_image\n",
    "        - Transform them into pure black and white images\n",
    "        - Save them in a 'music piece' subfolder of the 'output_path' folder\n",
    "       \n",
    "    --> Input path: path to folder with input images (e.g., '../../data_test/Input_image')\n",
    "    --> Output path: path to folder where we wish to save output reshaped images (e.g., '../../data_test/Input_image_cleaned')\n",
    "    \"\"\"\n",
    "    \n",
    "    for music in os.listdir(input_path):\n",
    "        \n",
    "        output_folder = f'{output_path}/{music}' # Creating one sub_folder for each music piece in the 'output_path' folder\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        for image in os.listdir(f\"{input_path}/{music}\"):\n",
    "            image_path = f'{input_path}/{music}/{image}'\n",
    "            image_read = Image.open(image_path) # Reading each image\n",
    "            new_image = image_read.resize((106,106)) # Resizing each image\n",
    "            new_image = new_image.convert(\"1\") # Convert each image to pure black and white\n",
    "            new_image.save(f'{output_folder}/{image}') # Saving each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "clean_images('../../data_test/Input_image/', '../../data_test/Input_image_cleaned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "get_clean_midi_data_as_images('../../data_raw/', '../../data_image/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "clean_images('../../data_image/', '../../data_image_cleaned/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images to be used are now stored in the 'data_image_cleaned' folder   \n",
    "Images are all of shape 106x106 and converted to pure black & white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get clean array dataset from clean image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_array(input_path): \n",
    "    \"\"\"\n",
    "    Generate an array containing all images from 'input_path' folder in array format\n",
    "    \n",
    "    --> input_path = path of the folder containing clean images (e.g., '../../data_image_cleaned') \n",
    "    \"\"\"\n",
    "    \n",
    "    pixels = []\n",
    "    for music in os.listdir(input_path):\n",
    "        for image in os.listdir(f\"{input_path}/{music}\"):\n",
    "            image_path = f'{input_path}/{music}/{image}'\n",
    "            image_read = Image.open(image_path) # Reading each image\n",
    "            pixels_image = np.array(image_read.getdata()).astype('float32') # Store all pixel values in an array, each i_th-sequence contains the values of pixels in a i_th-row\n",
    "            pixels_image = pixels_image / 255.0 # All the values are 0 (black) and white (255). Normalize pixel values to be between 0 and 1\n",
    "            pixels.append(pixels_image.reshape(106, 106,1)) # Reshape pixels to be a matrix\n",
    "\n",
    "    pixels = np.array(pixels)\n",
    "    \n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1507, 106, 106, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing the function for the entire dataset\n",
    "input_path = '../../data_image_cleaned'\n",
    "pixels = get_pixels_array(input_path)\n",
    "pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_from_pixels(pixels_matrix, image_number):\n",
    "    \"\"\"\n",
    "    Given a pixel matrix representing a dataset, get representation of one image (number image_number)\n",
    "    \"\"\"\n",
    "    plt.imshow(np.squeeze(pixels_matrix[image_number, :, :, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 417,
       "width": 425
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image_from_pixels(pixels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is now given in the form of a matrix containing 1507 images of shape 106 x 106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input,Dense, Reshape, Flatten, BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Dropout\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 106\n",
    "image_length = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standalone discriminator model\n",
    "def def_discriminator(in_shape=(image_height,image_length,1)): # Input is an image of shape 106 x 106 in black and white\n",
    "    \"\"\"\n",
    "    Returns a compiled discriminator model\n",
    "    \"\"\"\n",
    " \n",
    "    # Define inputs\n",
    "    inputs = Input(in_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    convolutional_layer_1 = Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=in_shape) (inputs)\n",
    "    activation_1 = LeakyReLU(alpha=0.2) (convolutional_layer_1)\n",
    "    dropout_1 = Dropout(0.5) (activation_1)\n",
    "    \n",
    "    # Block 2\n",
    "    convolutional_layer_2 = Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=in_shape) (dropout_1)\n",
    "    activation_2 = LeakyReLU(alpha=0.2) (convolutional_layer_2)\n",
    "    dropout_2 = Dropout(0.5) (activation_2)\n",
    "    \n",
    "    # Classifier\n",
    "    flattened_layer = Flatten()(dropout_2)\n",
    "    batch_normalization_layer = BatchNormalization()(flattened_layer)\n",
    "    output_discriminator = Dense(1, activation=\"sigmoid\")(batch_normalization_layer)\n",
    "    \n",
    "    # Defining discrimnator model\n",
    "    model = Model(inputs, outputs = output_discriminator)\n",
    "    \n",
    "    # Compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 106, 106, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 53, 53, 64)        640       \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 53, 53, 64)        0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 53, 53, 64)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 27, 27, 64)        36928     \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 27, 27, 64)        0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 27, 27, 64)        0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 46656)             0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 46656)             186624    \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 46657     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270849 (1.03 MB)\n",
      "Trainable params: 177537 (693.50 KB)\n",
      "Non-trainable params: 93312 (364.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_model = def_discriminator()\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimension = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standalone generator model\n",
    "def def_generator(in_shape = latent_dimension):\n",
    "    \"\"\"\n",
    "    Returns a generator model WITHOUT compiling it\n",
    "    \"\"\"\n",
    "    # Defining inputs\n",
    "    inputs = Input(in_shape)\n",
    "    \n",
    "    # Block 1 - foundation for 53 x 53 images\n",
    "    n_nodes_1 = 128 * 53 * 53\n",
    "    dense_1 = Dense(n_nodes_1, input_dim=latent_dimension)(inputs)\n",
    "    activation_1 = LeakyReLU(alpha=0.2)(dense_1)\n",
    "    reshape_layer = Reshape( (53,53,128))(activation_1)\n",
    "    \n",
    "    # Block 2\n",
    "    dense_2 = Dense(1024)(reshape_layer)\n",
    "    conv2d_transposed_layer_1 = Conv2DTranspose(1024,(4,4), strides=(2,2), padding=\"same\")(dense_2)\n",
    "    \n",
    "    # Block 3\n",
    "    dense_3 = Dense(1024)(conv2d_transposed_layer_1)\n",
    "    activation_2 = LeakyReLU(alpha=0.2)(dense_3)\n",
    "    dense_4 = Dense(1024)(activation_2)\n",
    "    conv2d_transposed_layer_1 = Conv2DTranspose(1,(7,7), padding=\"same\", activation='sigmoid')(dense_4)\n",
    "    \n",
    "    # Generate model\n",
    "    model = Model(inputs, outputs=conv2d_transposed_layer_1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 359552)            36314752  \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 359552)            0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 53, 53, 128)       0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 53, 53, 1024)      132096    \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 106, 106, 1024)    16778240  \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 106, 106, 1024)    1049600   \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 106, 106, 1024)    0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 106, 106, 1024)    1049600   \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 106, 106, 1)       50177     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55374465 (211.24 MB)\n",
      "Trainable params: 55374465 (211.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model = def_generator()\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined generator and discriminator model, for updating the generator\n",
    "def def_gan(generator, discriminator):\n",
    "    \"\"\"\n",
    "    Returns a compiled GAN model\n",
    "    \"\"\"\n",
    "    # Make weights in the discriminator not trainable - train only generator weights\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # Instantiate GAN model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add generator and discrimnator\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    # Compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_2 (Functional)        (None, 106, 106, 1)       55374465  \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 270849    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55645314 (212.27 MB)\n",
      "Trainable params: 55374465 (211.24 MB)\n",
      "Non-trainable params: 270849 (1.03 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_model = def_gan(generator_model, discriminator_model)\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require one batch (or a half) batch of real images from the dataset for each update to the GAN model. A simple way to achieve this is to select a random sample of images from the dataset each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of 'real' images\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Takes as input cleaned dataset and a number of samples to be generated\n",
    "    Returns a random sample of n_samples images and their corresponding label (=1 because these are real images)\n",
    "    \"\"\"\n",
    "    # Choose random instances (i.e., randomly select n_samples indexes from dataset)\n",
    "    iX = np.random.randint(0, pixels.shape[0], n_samples)\n",
    "    # Loading corresponding images\n",
    "    X = pixels[iX]\n",
    "    # Creating corresponding 'Real' (=1) labels\n",
    "    y = np.ones((n_samples, 1)) \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs for the generator model are random points from latent space corresponding to a Gaussian distributed variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points in latent space as input for the generator, following Gaussian distributed variable\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # Generate (latent dimension x n_samples) array of random values taken from x axis of Normal Distribution\n",
    "    x_input = np.random.randn(latent_dim*n_samples)\n",
    "    # Reshape to have num_samples entries for each one with latent_dimension values\n",
    "    x_input = x_input.reshape(n_samples, latent_dimension)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # Generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # Predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # Generate corresponding 'Fake' (=0) class labels\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 250\n",
    "n_batch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to display accuracy of the Discriminator in preidtcing correctly both real and fake music samples\n",
    "def show_current_discriminator_accuracy(discriminator_model, generator_model, pixels, latent_dimension = 100, num_samples_to_test = 100):\n",
    "    \n",
    "    #generate real music samples\n",
    "    X_real, y_real = generate_real_samples(pixels, num_samples_to_test)\n",
    "  \n",
    "    #generate fake music samples\n",
    "    X_fake, y_fake = generate_fake_samples(generator_model, latent_dimension, num_samples_to_test)\n",
    "    \n",
    "    #evaluate the accuracy of the discriminator on real music samples\n",
    "    _, accuracy_on_real = discriminator_model.evaluate(X_real, y_real, verbose=0)\n",
    "    #evaluate the accuracy of the discriminator on fake music samples\n",
    "    _, accuracy_on_fake = discriminator_model.evaluate(X_fake, y_fake, verbose=0)\n",
    "\n",
    "    #print results\n",
    "    print(\"   Current accuracy of the discriminator on real music samples:\", round(accuracy_on_real*100,3),\"%\")\n",
    "    print(\"   Current accuracy of the discriminator on fake music samples:\", round(accuracy_on_fake*100,3),\"% \\n\")\n",
    "\n",
    "    return accuracy_on_real, accuracy_on_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN model\n",
    "def train(generator_model, discriminator_model, gan_model, dataset, latent_dim = 100, n_epochs=250, n_batch=15):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch) # Number of batches per epoch\n",
    "    half_batch = int(n_batch / 2) # Half the number of batches \n",
    "    \n",
    "    # Storing results along the epochs\n",
    "    # Dataframe containing loss and accuracy for each epoch\n",
    "    discriminator_info_per_epoch = pd.DataFrame(columns=['loss_discriminator_on_real_music', 'loss_generator_on_fake_music', 'accuracy_on_real', 'accuracy_on_fake'])\n",
    "    \n",
    "    accuracy_on_fake = 0\n",
    "    accuracy_on_real = 0\n",
    "    \n",
    "    # For each epoch\n",
    "    for i in range(n_epochs): # Enumerate epochs\n",
    "        \n",
    "        # For each batch\n",
    "        for j in range(bat_per_epo):\n",
    " \n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = discriminator_model.train_on_batch(X_real, y_real)\n",
    "            \n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(generator_model, latent_dim, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = discriminator_model.train_on_batch(X_fake, y_fake)\n",
    "            \n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "        # print the current accuracy obtained in classifing correctly both real music samples and fake music samples (the ones generated by the generator)\n",
    "        accuracy_on_real, accuracy_on_fake = show_current_discriminator_accuracy(discriminator_model, generator_model, pixels, latent_dimension)\n",
    "        discriminator_info_per_epoch = discriminator_info_per_epoch.append({'loss_discriminator_on_real_music':  d_loss1, 'loss_generator_on_fake_music': g_loss, 'accuracy_on_real': accuracy_on_real, 'accuracy_on_fake': accuracy_on_fake}, ignore_index=True)\n",
    "        \n",
    "    return gan_model, discriminator_info_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 8s 8s/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(generator_model, discriminator_model, gan_model, pixels)\n",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 52\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m d_loss1, _ \u001b[39m=\u001b[39m discriminator_model\u001b[39m.\u001b[39mtrain_on_batch(X_real, y_real)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# generate 'fake' examples\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m X_fake, y_fake \u001b[39m=\u001b[39m generate_fake_samples(generator_model, latent_dim, half_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# update discriminator model weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m d_loss2, _ \u001b[39m=\u001b[39m discriminator_model\u001b[39m.\u001b[39mtrain_on_batch(X_fake, y_fake)\n",
      "\u001b[1;32m/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb Cell 52\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m x_input \u001b[39m=\u001b[39m generate_latent_points(latent_dim, n_samples)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Predict outputs\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m X \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpredict(x_input)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Generate corresponding 'Fake' (=0) class labels\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/agatheduranton/code/AgatheDuranton/ChopAI/ChopAI/ml_logic/full-data-preprocessing.ipynb#Y130sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n_samples, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/keras/src/engine/training.py:2554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2552\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   2553\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2554\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   2555\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2556\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/ChopAI/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(generator_model, discriminator_model, gan_model, pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChopAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
